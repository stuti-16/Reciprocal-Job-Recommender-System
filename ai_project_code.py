# -*- coding: utf-8 -*-
"""AI_PROJECT_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KqcyWgnocx_IJX8Nd_iZkp0h753zQX3a

Extracting data from resume using Resume Parser Library</h2>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install nltk
# !pip install spacy==2.3.5
# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt-get -qq install -y graphviz && pip install pyresparser
# !apt-get -qq install -y graphviz && pip install python_docx

import numpy as np
import pandas as pd
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from pyresparser import ResumeParser
import os
from docx import Document

# %%capture
# !git clone https://github.com/jsvine/pdfplumber.git

# %%capture
# !cd pdfplumber; pip install .; cd ..

# %%capture
# !pip3 install pdfplumber

from google.colab import drive
drive.mount('/content/drive')

"""# JOB SEEKER

**Upload Resume Module**
"""

from google.colab import files
uploaded = files.upload()
# type(uploaded)

type(uploaded)
next(iter(uploaded))

# save resume file in drive

import shutil
colab_link = "/content/"+next(iter(uploaded))
gdrive_link = "/content/drive/MyDrive/Colab/AI_project/resumes"
filed = shutil.copy(colab_link, gdrive_link)

try:
    doc = Document()
    # if file is in .txt format
    with open(filed, 'r') as file:
        doc.add_paragraph(file.read())
    doc.save("text.docx")
    data = ResumeParser('text.docx').get_extracted_data()
except:
    data = ResumeParser(filed).get_extracted_data()

print(type(data))

# listd1 = []
# for i in data.keys():
#   listd1.append(i)
#   # print(i+",", end='')

# listd1

print(data)

# d1 ={}
counter =0
for key, val in data.items():
    print(key," : ",val)
    # d1.update( {key : listd1[counter]} )
    # counter = counter+1

from csv import DictWriter
 
# list of column names
field_names = data.keys()
 
# Open CSV file in append mode
# Create a file object for this file
with open('/content/drive/MyDrive/Colab/AI_project/resumes/resume_dataset.csv', 'a') as f_object:
  # TO ADD COLUMNS NAMES
    # dictwriter_object = DictWriter(f_object, fieldnames=field_names)
    # dictwriter_object.writerow(d1)
    
    # Pass the file object and a list of column names to DictWriter()
    # You will get a object of DictWriter
    dictwriter_object = DictWriter(f_object, fieldnames=field_names)
 
    # Pass the dictionary as an argument to the Writerow()
    dictwriter_object.writerow(data)
 
    # Close the file object
    f_object.close()

df =pd.read_csv('/content/drive/MyDrive/Colab/AI_project/resumes/resume_dataset.csv')
df.tail()

"""Skill extraction from job description"""

stopw  = set(stopwords.words('english'))
type(stopw)
# stopw

all_jobs_df =pd.read_csv('https://raw.githubusercontent.com/pik1989/HRAnalytics-Project/main/job_final.csv') 
print("Size of the data set : ", all_jobs_df.shape)
all_jobs_df.head()

all_jobs_df['Job_Description']

print(all_jobs_df.columns)
all_jobs_df.rename(columns = {'Unnamed: 0':'index'}, inplace = True)

all_jobs_df = all_jobs_df.dropna()
print("Size of the data set : ", all_jobs_df.shape)

import spacy

# load english language model and create nlp object from it
nlp = spacy.load("en_core_web_sm") 

def preprocess(text):
    # remove stop words and lemmatize the text
    doc = nlp(text)
    filtered_tokens = []
    for token in doc:
        if token.is_stop or token.is_punct:
            continue
        filtered_tokens.append(token.lemma_)
    
    return " ".join(filtered_tokens)

# print(all_jobs_df['Job_Description'][0])
# print("")
# print("")
# print("-----------------------------------AFTER PROCESSING------------------------------------")
# print("Processed Data :",end='')
# processes_jobdes = preprocess(all_jobs_df['Job_Description'][0])
# print(processes_jobdes)

from sklearn.feature_extraction.text import CountVectorizer

# from sklearn.feature_extraction.text import CountVectorizer

# vtest = CountVectorizer()
# vtest = CountVectorizer(ngram_range=(1,2))
# vtest.fit(["I am a robot who loves to play with people"])
# print("Length of frequency matrix :",len(vtest.vocabulary_))
# count = 1
# print(" ")
# for key, val in vtest.vocabulary_.items():
#   if count%5:
#     print(key,":", val, end=" ")
#   else:
#     print(key,":", val)
#   count = count+1
# tf_count = vtest.transform(["I am a robot"])
# print(" ")
# print("Count Vector for sentence 'I am a robot' :")
# tf_count.toarray()

# v = CountVectorizer(ngram_range=(1,2))
# v.fit([processes_jobdes])
# print(type(v.vocabulary_))
# print(len(v.vocabulary_))
# v.vocabulary_

all_jobs_df['test']=all_jobs_df['Job_Description'].apply(preprocess)
all_jobs_df['test']

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install ftfy

import re

from ftfy import fix_text

def string_process(string, n=2):
    string = fix_text(string) # fix text
    string = string.encode("ascii", errors="ignore").decode() #remove non ascii chars 
    chars_to_remove = [")","(",".","|","[","]","{","}","'"]
    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'
    string = re.sub(rx, '', string)
    string = string.replace('&', 'and')
    string = string.replace('\n', ' ')
    string = string.replace(',', ' ')
    string = string.replace('-', ' ')
    string = string.title()                       # normalise case - capital at start of each word
    string = re.sub(' +',' ',string).strip()      # get rid of multiple spaces and replace with a single
    string = ' '+ string +' '                     # pad names for ngrams...
    string = re.sub(r'[,-./]|\sBD',r'', string)
    string = string.lower()
    return string

# string_process(all_jobs_df['Job_Description'][2])

all_jobs_df['test']=all_jobs_df['Job_Description'].apply(string_process)
all_jobs_df['test']

# print(data['skills'])

data_lower = []
for s in data['skills']:
    data_lower.append(s.lower())

jobseeker_skills = " "
jobseeker_skills = jobseeker_skills.join(data_lower)
j=1
for i in jobseeker_skills:
  if j%40:
    print(i, end='')
  else:
    print(i)
  j = j+1

v = CountVectorizer()

v = CountVectorizer(ngram_range=(1,2))
v.fit([jobseeker_skills])
print(type(v.vocabulary_))
print(len(v.vocabulary_))
v.vocabulary_

tf_count = v.transform(all_jobs_df['test'])
tf_ud = tf_count.toarray()
print(type(tf_count))
print(tf_ud.shape)
print(tf_ud)

from sklearn.neighbors import NearestNeighbors

nbrs = NearestNeighbors(n_neighbors=10, n_jobs=-1).fit(tf_count)

def getNearestN(query):
  q_tf_count = v.transform(query)
  distances, indices = nbrs.kneighbors(q_tf_count)
  return distances, indices

distances, indices = getNearestN([jobseeker_skills])

print("Distances          ", "Indices")
for i in range(1,len(distances[0])):
  print(distances[0][i],"    ", indices[0][i])

type(indices)
for i in indices:
    rec_jobs = all_jobs_df[['Position', 'Company','Location']].iloc[i] 
rec_jobs

"""**Using TF_IDF vectorizer**"""

from sklearn.feature_extraction.text import TfidfVectorizer
import re

vtest = TfidfVectorizer()
vtest = TfidfVectorizer(ngram_range=(1,2))
vtest.fit(["I am a robot who loves to play with people"])
print("Length of frequency matrix :",len(vtest.vocabulary_))
count = 1
print(" ")
for key, val in vtest.vocabulary_.items():
  if count%5:
    print(key,":", val, end=" ")
  else:
    print(key,":", val)
  count = count+1
tf_count = vtest.transform(["I am a robot"])
print(" ")
print("Count Vector for sentence 'I am a robot' :")
tf_count.toarray()

vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
vectorizer.fit([jobseeker_skills])
print(type(vectorizer.vocabulary_))
print(len(vectorizer.vocabulary_))
vectorizer.vocabulary_

tfidf = vectorizer.transform(all_jobs_df['test'])
print(type(tfidf))
tfidf_arr = tfidf.toarray()
print(tfidf_arr.shape)

print(vectorizer.get_feature_names_out())
print(tfidf.indices)
print(tfidf_arr)

nbrs = NearestNeighbors(n_neighbors=10, n_jobs=-1).fit(tfidf)

def getNearestN(query):
  q_tf_count = vectorizer.transform(query)
  distances, indices = nbrs.kneighbors(q_tf_count)
  return distances, indices

jobseeker_skills

distances, indices = getNearestN([jobseeker_skills])

print("Distances            ", "Indices")
for i in range(1,len(distances[0])):
  print(distances[0][i],"    ", indices[0][i])

type(indices)
for i in indices:
    rec_jobs = all_jobs_df[['Position', 'Company','Location']].iloc[i] 
rec_jobs

"""# JOB RECRUITOR

UPLOAD JOB DETAILS
"""

job_details = {}

job_details['company_name'] = input("Enter the company name: ")
job_details['job_title']  = input("Enter the job title: ")
job_details['job_des'] = input("Enter the job description: ")
job_details['job_location'] = input("Enter the job location: ")
job_details['exp_req']= input("Enter the experience required from candidate: ")
job_details['edu_req']= input("Enter the educational qualifications required: ")
job_details['cert_req']= input("Enter the exra certifications required (optional): ")

"""APPEND JOB DETAILS"""

from csv import DictWriter
 
field_names = job_details.keys()
 
with open('/content/drive/MyDrive/AI_project/job_sample/job_dataset.csv', 'a') as f_object:
  # TO ADD COLUMNS NAMES
    # dictwriter_object = DictWriter(f_object, fieldnames=field_names)
    # dictwriter_object.writerow(d1)
    
    # Pass the file object and a list of column names to DictWriter()
    # You will get a object of DictWriter
    dictwriter_object = DictWriter(f_object, fieldnames=field_names)
 
    # Pass the dictionary as an argument to the Writerow()
    dictwriter_object.writerow(job_details)
 
    # Close the file object
    f_object.close()

# listd1 = []
# for i in job_details.keys():
#   listd1.append(i)
#   # print(i+",", end='')

# listd1

# d1 ={}
# counter =0
# for key, val in job_details.items():
#     print(key," : ",val)
#     d1.update( {key : listd1[counter]} )
#     counter = counter+1

# d1

jobs =pd.read_csv('/content/drive/MyDrive/Colab/AI_project/job_sample/jobs_dataset.csv')
jobs.tail()

"""Skill extraction from job description"""

resumes =pd.read_csv('/content/drive/MyDrive/Colab/AI_project/resumes/resume_dataset.csv') 
resumes.tail()

resumes['skills']

print(resumes.columns)
# all_jobs_df.rename(columns = {'Unnamed: 2':'index'}, inplace = True)

# jobs_df = jobs.dropna()
resumes = resumes[resumes['skills'].notna()]
print("Size of the data set : ", resumes.shape)

resumes['test']=resumes['skills'].apply(preprocess)
resumes['test']

resumes['test']=resumes['skills'].apply(string_process)
resumes['test']

"""RECOMMENDING TOP 3 JOB-SEEKERS FOR A JOB"""

test_job = jobs.iloc[0]
type(test_job)

test_job

data_lower = []
for s in test_job['job_des']:
    data_lower.append(s.lower())

test_job_des = ""
test_job_des = test_job_des.join(data_lower)
j=1
for i in test_job_des:
  if j%40:
    print(i, end='')
  else:
    print(i)
  j = j+1

v = CountVectorizer()

v = CountVectorizer(ngram_range=(1,2))
v.fit([test_job_des])
print(type(v.vocabulary_))
print(len(v.vocabulary_))
v.vocabulary_

tf_count = v.transform(resumes['test'])
tf_ud = tf_count.toarray()
print(type(tf_count))
print(tf_ud.shape)
print(tf_ud)

nbrs = NearestNeighbors(n_neighbors=3, n_jobs=-1).fit(tf_count)

def getNearestN(query):
  q_tf_count = v.transform(query)
  distances, indices = nbrs.kneighbors(q_tf_count)
  return distances, indices

distances, indices = getNearestN([test_job_des])

len(distances[0])

print("Distances          ", "Indices")
for i in range(0,len(distances[0])):
  print(distances[0][i],"    ", indices[0][i])

type(indices)
for i in indices:
    rec_jobs = resumes.iloc[i] 
rec_jobs

"""# JOB TO JOB MATCHING"""

jobs.info()

jobs.head()

jobs = jobs[jobs['job_des'].notna()]
jobs.shape

jobs['job_des']

# CREATING A BAG OF WORDS

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert a collection of raw documents to a matrix of TF-IDF features.
# tf-idf means term-frequency times inverse document-frequency
# tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t)
# idf is computed as idf(t) = log [ n / df(t) ] + 1 

tdif = TfidfVectorizer(stop_words='english')
# print(type(tdif))
tdif_matrix = tdif.fit_transform(jobs['job_des'])
tdif_matrix.shape

print(type(tdif_matrix))

from sklearn.metrics.pairwise import linear_kernel

# considers not only the similarity between vectors under the same dimension,
# but also across dimensions. When used in machine learning algorithms, 
# this allows to account for feature interaction.

cosine_sim = linear_kernel(tdif_matrix, tdif_matrix)

#  tf-idf functionality in sklearn.feature_extraction.text can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel, only slower

indices = pd.Series(jobs.index, index=jobs['job_title']).drop_duplicates()
indices

def get_recommendations(title,cosine_sim=cosine_sim):
  idx = indices[title]
  # print(idx)
  sim_scores = list(enumerate(cosine_sim[idx]))
  # print(sim_scores)
  sim_scores = sorted(sim_scores, key=lambda X:X[1], reverse=True)
  # we don't how this 'key=lambda X:X[1]' works
  # print(sim_scores)
  sim_scores = sim_scores[0:5]

  tech_indices = [i[0] for i in sim_scores]
  return jobs.iloc[tech_indices]

sim_jobs = get_recommendations('Software Testing Internship')
sim_jobs



"""# RESUME TO RESUME MATCHING"""

resumes.info()

resumes.head()

resumes = resumes[resumes['skills'].notna()]
resumes.shape

resumes['test']

# CREATING A BAG OF WORDS

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert a collection of raw documents to a matrix of TF-IDF features.
# tf-idf means term-frequency times inverse document-frequency
# tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t)
# idf is computed as idf(t) = log [ n / df(t) ] + 1 

tdif = TfidfVectorizer(stop_words='english')
# print(type(tdif))
tdif_matrix = tdif.fit_transform(resumes['test'])
tdif_matrix.shape

print(type(tdif_matrix))

from sklearn.metrics.pairwise import linear_kernel

# considers not only the similarity between vectors under the same dimension,
# but also across dimensions. When used in machine learning algorithms, 
# this allows to account for feature interaction.

cosine_sim = linear_kernel(tdif_matrix, tdif_matrix)

#  tf-idf functionality in sklearn.feature_extraction.text can produce 
# normalized vectors, in which case cosine_similarity is equivalent to
#  linear_kernel, only slower

indices = pd.Series(resumes.index, index=resumes['name']).drop_duplicates()
indices

def get_recommendations(title,cosine_sim=cosine_sim):
  idx = indices[title]
  # print(idx)
  sim_scores = list(enumerate(cosine_sim[idx]))
  # print(sim_scores)
  sim_scores = sorted(sim_scores, key=lambda X:X[1], reverse=True)
  # we don't how this 'key=lambda X:X[1]' works
  # print(sim_scores)
  sim_scores = sim_scores[1:4]

  tech_indices = [i[0] for i in sim_scores]
  return resumes.iloc[tech_indices]

sim_res = get_recommendations('Lakshay Yadav')
sim_res

"""# COLABORATIVE FILTERING"""

sim_res['test']

sim_skills = ""
for res in sim_res['test']:
  sim_skills = sim_skills +" "+ res

sim_skills

v = CountVectorizer()

v = CountVectorizer(ngram_range=(1,2))
v.fit([sim_skills])
print(type(v.vocabulary_))
print(len(v.vocabulary_))
v.vocabulary_

tf_count = v.transform(jobs['test'])
tf_ud = tf_count.toarray()
print(type(tf_count))
print(tf_ud.shape)
print(tf_ud)

nbrs = NearestNeighbors(n_neighbors=5, n_jobs=-1).fit(tf_count)

def getNearestN(query):
  q_tf_count = v.transform(query)
  distances, indices = nbrs.kneighbors(q_tf_count)
  return distances, indices

distances, indices = getNearestN([jobseeker_skills])

print("Distances          ", "Indices")
for i in range(1,len(distances[0])):
  print(distances[0][i],"    ", indices[0][i])

"""TOP N JOBS RECOMMENDED TO LAKSHAY YADAV USING COLLABORATIVE FILTERING"""

type(indices)
for i in indices:
    rec_jobs = jobs[['job_title', 'company_name','job_location']].iloc[i] 
rec_jobs